server:
  host: "0.0.0.0"
  port: 8000

llama:
  url: "http://localhost:8081"

tilde:
  url: "http://localhost:8081"

database:
  path: "data/ulfweb.db"

defaults:
  temperature: 0.7
  top_k: 40
  top_p: 0.9
  repeat_penalty: 1.1
  max_tokens: 2048
  system_prompt: "You are a helpful assistant."
  model: ""

models:
  path: "/home/karlth/llama.cpp/models"
  llama_server: "/home/karlth/llama.cpp/build/bin/llama-server"
